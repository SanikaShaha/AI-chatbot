# Conversational UI Chatbot App with Gemini, LangChain and Streamlit

Here we will build a advanced Conversational UI-based chatbot using LangChain and Streamlit with the following features:

- Custom Landing Page
- Conversational memory

## Install App and LLM dependencies

!pip install langchain==0.1.12 -q
!pip install langchain-google-genai==0.0.7 -q
!pip install langchain-community==0.0.29 -q
!pip install streamlit==1.32.2 -q
!pip install pyngrok==7.1.5 -q
!pip install google-generativeai>=0.3.2 -q

## Load Gemini API Credentials

Here we load it from a file so we don't explore the credentials on the internet by mistake

import os
from google.colab import userdata
os.environ['GEMINI_API_KEY'] = userdata.get('GEMINIAI')

## Basic Streamlit UI without Memory

This simple version shows how to:

1. Create a basic Streamlit interface
2. Connect directly to Gemini
3. Process basic Q&A without memory

%%writefile basic_app.py

import streamlit as st
from langchain_google_genai import ChatGoogleGenerativeAI

st.title("Basic AI Assistant (No Memory)")

# Initialize the LLM
gemini = ChatGoogleGenerativeAI(model='gemini-2.0-flash-001',
                               temperature=0.1)

# Simple input/output
if user_input := st.chat_input("Ask a question"):
    st.chat_message("human").write(user_input)

    with st.chat_message("ai"):
        response = gemini.invoke(user_input)
        st.write(response.content)

## Start the app

!streamlit run basic_app.py --server.port=8989 &>./logs.txt &

from pyngrok import ngrok

# Terminate open tunnels if exist
ngrok.kill()

ngrok.set_auth_token(userdata.get('N'))

# Open an HTTPs tunnel on port XXXX which you get from your `logs.txt` file
ngrok_tunnel = ngrok.connect(8989)
print("Streamlit App:", ngrok_tunnel.public_url)

## Remove running app processes

ngrok.kill()

!ps -ef | grep streamlit

!sudo kill -9 1616

## Part 2: Showing Messages with Manual Memory

This demonstrates:

1. How to manually implement memory in Streamlit
2. The challenge of formatting context for the LLM
3. Why specialized tools like LangChain help

%%writefile manual_app.py

import streamlit as st
from langchain_google_genai import ChatGoogleGenerativeAI

st.title("AI Assistant with Manual Memory")

# Initialize the LLM
gemini = ChatGoogleGenerativeAI(model='gemini-2.0-flash-001',
                               temperature=0.1)

# Initialize session state for memory
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display existing messages
for message in st.session_state.messages:
    st.chat_message(message["role"]).write(message["content"])

# Process new input
if user_input := st.chat_input("Ask a question"):
    # Add user message to history
    st.session_state.messages.append({"role": "human", "content": user_input})
    st.chat_message("human").write(user_input)

    # Manually create a context string from history
    context = "Previous conversation:\n"
    for msg in st.session_state.messages:
        context += f"{msg['role']}: {msg['content']}\n"

    with st.chat_message("ai"):
        # Send context + new question
        full_prompt = context + "\nPlease respond to the last question."
        response = gemini.invoke(full_prompt)
        st.write(response.content)

        # Add AI response to history
        st.session_state.messages.append({"role": "ai", "content": response.content})

!streamlit run manual_app.py --server.port=8989 &>./logs.txt &

from pyngrok import ngrok

# Terminate open tunnels if exist
ngrok.kill()

ngrok.set_auth_token(userdata.get('N'))

# Open an HTTPs tunnel on port XXXX which you get from your `logs.txt` file
ngrok_tunnel = ngrok.connect(8989)
print("Streamlit App:", ngrok_tunnel.public_url)

ngrok.kill()

!ps -ef | grep streamlit

!sudo kill -9 4087

## Part 3: Full LangChain Integration

context = """
HEALTH CARE and life cycle , technology creating a complete ecosystem and a paradigm shift in virtually every sector of the technical industry, academics and research. Artificial Intelligence and Data Science is the future of technology which are changing the world at very high pace. The basic objectives of this course is to train students with the next age of Intelligence and analytics generated by machines, influencing nearly every facet of our lives to help improve efficiencies and augment human capabilities, influencing consumer products with significant breakthroughs in healthcare, manufacturing, finance and retail industries.
With the tremendous amount of data generated every day and the computing power available, Data Science plays important role helping every business organisation in identifying business trends and changes through advanced Big Data Analytics with variety of techniques and tools to interpret and predict business results and future from multiple data sources through statistical analysis, data aggregation, and data mining. Artificial Intelligence and Data Science undergraduate engineering course has been started by the institute from the academic year July 2020 with the intake capacity of 60 seats.
"""

%%writefile langchain_app.py

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from operator import itemgetter
import streamlit as st
import requests
from bs4 import BeautifulSoup

# Customize initial app landing page
st.set_page_config(page_title="AI Assistant", page_icon="ðŸ¤–")
st.title("Welcome I am AI Assistant ðŸ¤–")

# Load a connection to Gemini LLM
gemini = ChatGoogleGenerativeAI(model='gemini-2.0-flash-001',
                               temperature=0.1,
                               convert_system_message_to_human=True)

def scrape_website_text(url):
    # Set up headers to mimic a browser request
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    try:
        # Send GET request to the website
        response = requests.get(url, headers=headers)

        # Check if request was successful
        if response.status_code == 200:
            # Parse HTML content
            soup = BeautifulSoup(response.text, 'html.parser')

            # Remove script and style elements that might contain text we don't want
            for script_or_style in soup(['script', 'style', 'head', 'title', 'meta', '[document]']):
                script_or_style.decompose()

            # Get all text from the page
            text = soup.get_text()

            # Clean up the text by removing extra whitespace
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = '\n'.join(chunk for chunk in chunks if chunk)

            return text
        else:
            return f"Failed to retrieve the webpage. Status code: {response.status_code}"

    except Exception as e:
        return f"An error occurred: {str(e)}"

url = "https://www.geeksforgeeks.org/courses/data-science-live?itm_source=geeksforgeeks&itm_medium=gfg_submenu&itm_campaign=DS_Submenu"

scraped_text = scrape_website_text(url)

# Add a basic system prompt for LLM behavior
SYS_PROMPT = """
              Act as a helpful assistant of KJSIT for AI and Data science program queries and answer questions to the best of your ability.
              Do not make up answers.
              """

final_sys_prompt = SYS_PROMPT + scraped_text

# Create a prompt template for langchain to use history to answer user prompts
prompt = ChatPromptTemplate.from_messages(
  [
    ("system", final_sys_prompt),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{input}"),
  ]
)

# Create a basic llm chain
llm_chain = (
  prompt
  |
  gemini
)

# Store conversation history in Streamlit session state
streamlit_msg_history = StreamlitChatMessageHistory()

# Create a conversation chain
conversation_chain = RunnableWithMessageHistory(
  llm_chain,
  lambda session_id: streamlit_msg_history,  # Accesses memory
  input_messages_key="input",
  history_messages_key="history",
)

# Render current messages from StreamlitChatMessageHistory
for msg in streamlit_msg_history.messages:
  st.chat_message(msg.type).write(msg.content)

# If user inputs a new prompt, display it and show the response
if user_prompt := st.chat_input():
  st.chat_message("human").write(user_prompt)
  # This is where response from the LLM is shown
  with st.chat_message("ai"):
    config = {"configurable": {"session_id": "any"}}
    # Get llm response
    response = conversation_chain.invoke({"input": user_prompt}, config)
    st.markdown(response.content) # Display response directly

## Start the app

!streamlit run langchain_app.py --server.port=8989 &>./logs.txt &

from pyngrok import ngrok

# Terminate open tunnels if exist
ngrok.kill()

ngrok.set_auth_token(userdata.get('N'))

# Open an HTTPs tunnel on port XXXX which you get from your `logs.txt` file
ngrok_tunnel = ngrok.connect(8989)
print("Streamlit App:", ngrok_tunnel.public_url)

## Remove running app processes

ngrok.kill()

!ps -ef | grep streamlit

!sudo kill -9 7560

import requests
from bs4 import BeautifulSoup

def scrape_website_text(url):
    # Set up headers to mimic a browser request
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    try:
        # Send GET request to the website
        response = requests.get(url, headers=headers)

        # Check if request was successful
        if response.status_code == 200:
            # Parse HTML content
            soup = BeautifulSoup(response.text, 'html.parser')

            # Remove script and style elements that might contain text we don't want
            for script_or_style in soup(['script', 'style', 'head', 'title', 'meta', '[document]']):
                script_or_style.decompose()

            # Get all text from the page
            text = soup.get_text()

            # Clean up the text by removing extra whitespace
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = '\n'.join(chunk for chunk in chunks if chunk)

            return text
        else:
            return f"Failed to retrieve the webpage. Status code: {response.status_code}"

    except Exception as e:
        return f"An error occurred: {str(e)}"

# # Example usage
# if __name__ == "__main__":
#     website_url = "https://example.com"  # Replace with your target website URL
#     extracted_text = scrape_website_text(website_url)
#     print(extracted_text)

#     # Optional: Save to file
#     with open("extracted_text.txt", "w", encoding="utf-8") as file:
#         file.write(extracted_text)

url = "https://www.geeksforgeeks.org/courses/data-science-live?itm_source=geeksforgeeks&itm_medium=gfg_submenu&itm_campaign=DS_Submenu"

scraped_text = scrape_website_text(url)

print(scraped_text)
